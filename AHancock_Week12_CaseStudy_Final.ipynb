{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "## MSDS 7333 - Section 401\n",
    "## Case Study Week 12\n",
    "[Data Science @ Southern Methodist University](https://datascience.smu.edu/)\n",
    "\n",
    "### Due:\n",
    "06 August 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "* [Team Members](#Team-Members)\n",
    "* [Abstract](#Abstract)\n",
    "* [Introduction](#Introduction)\n",
    "* [Methods](#Methods)\n",
    "* [Results](#Results)\n",
    "* [Conclusion](#Conclusion)\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Team-Members\"></a>Team Members\n",
    "* Kevin Cannon\n",
    "* Austin Hancock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Abstract\"></a>Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the Higgs Data Set is used to experiment and explore neural networks with the Keras API. Three different architectures, optimizers, and initializers are explored and scored to produce a final model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Introduction\"></a>Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of particle physics, or high-energy physics is to understand what are the most fundamental constituents of matter and how these particles interact. By discovering the most elementary constituents of matter and energy, exploring the basic nature of space and time itself, and probing the interactions between them, scientists work to understand how the universe works at its most elementary level.\n",
    "\n",
    "The primary tools of experimental high-energy physicists are particle accelerators, which collide protons and/or antiprotons to create particles that only occur at extremely high-energy densities. Advanced particle accelerators, particle detectors, and sophisticated computing techniques are essential tools for modern particle physics research. The advancement of dedicated technology for particle physics has benefited tremendously from progress in other areas of science. With the scarcity and difficulty in obtaining data, statistical and machine learning tools are invaluable tools for discovery of new insights into the nature of matter.\n",
    "\n",
    "In order to isolate highly dimensional particle collision data, the relative likelihood of a new particles is calculated. Since this likelihood is difficult to express analytically, collision data simulated using Monte Carlo methods are used as the basis for approximation. Machine learning classifiers, such as neural networks, provide a method to solve this computational problem.\n",
    "\n",
    "The Higgs Data Set used in our neural network exploration comes from a paper by Pierre Baldi, Peter Sadowski, and Daniel Whiteson from the University of California, Irvine. Published in July 2014 in Nature Communications, the paper “Searching for exotic particles in high-energy physics with deep learning” shows that deep-learning classification methods can improve results over other approaches. From the abstract of the paper:\n",
    "> Standard approaches have relied on ‘shallow’ machine-learning models that have a limited capacity to learn complex nonlinear functions of the inputs, and rely on a painstaking search through manually constructed nonlinear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Here, using benchmark data sets, we show that deep-learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8% over the best current approaches. This demonstrates that deep-learning approaches can improve the power of collider searches for exotic particles.\n",
    "\n",
    "We will use the data set to distinguish between a signal process which produces Higgs boson particles and a background process that does not produce the particles. The data was produced using Monte Carlo simulations, with 28 features and 11,000,000 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Methods\"></a>Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we will use the Keras API. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. TensorFlow, which is the backend used for this lab, is an open-source symbolic tensor manipulation framework developed by Google.\n",
    "\n",
    "Keras was developed with a focus on enabling fast experimentation. The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                1450      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6919 - acc: 0.5281\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 6s 6us/step - loss: 0.6918 - acc: 0.5284\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 6s 6us/step - loss: 0.6915 - acc: 0.5287\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 6s 6us/step - loss: 0.6729 - acc: 0.5750\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 6s 6us/step - loss: 0.6531 - acc: 0.6099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6739729324561039"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original model\n",
    "# Changed N from 10,500,000 to 1,050,000\n",
    "# 3 layers. Neurons: 50, 50, 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N=1050000. #Change this line adjust the number of rows. \n",
    "data=pd.read_csv(\"HIGGS.csv\",nrows=N,header=None)\n",
    "test_data=pd.read_csv(\"HIGGS.csv\",nrows=50000,header=None,skiprows=1050000)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y = np.array(data.loc[:,0])\n",
    "x = np.array(data.loc[:,1:])\n",
    "x_test = np.array(test_data.loc[:,1:])\n",
    "y_test = np.array(test_data.loc[:,0])\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work # 1 & 2: Architecture and Activation Function Exploration\n",
    "In this section, we select 3 different architectures and run the model to determine the scores. The sigmoid activation function will be used for all base models.\n",
    "\n",
    "Then, we switch the activation functions from Sigmoid to two different activations functions for each architecture.\n",
    "\n",
    "Ten basic activations function exist in the Keras API that can be used in the models:\n",
    "* softmax\n",
    "* elu\n",
    "* selu\n",
    "* softplus\n",
    "* softsign\n",
    "* relu\n",
    "* tanh\n",
    "* sigmoid\n",
    "* hard_sigmoid\n",
    "* linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture #1: Three layer model with sigmoid activation functions\n",
    "Our starting point will be a three layer model with 100, 100, and 1 neurons in each respective layer. From here, we can see the effect of adding or subtracting layers from our model from the ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 13,101\n",
      "Trainable params: 13,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.5246\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 0.6895 - acc: 0.5344\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 0.6599 - acc: 0.5995\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6494 - acc: 0.6150\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6447 - acc: 0.6210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68048853933017528"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 layers. Neurons: 100, 100, 1 \n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(100, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three layer model (100, 100, 1 neurons) with sigmoid activation functions produced an ROC score of 0.680."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture #1: Three layer model with softmax activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_162 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 13,101\n",
      "Trainable params: 13,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 18s 17us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 16s 15us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 16s 15us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 16s 15us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 16s 16us/step - loss: 7.4990 - acc: 0.5296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change activation to \"softmax\"\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(100, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three layer model (100, 100, 1 neurons) with softmax activation functions produced an ROC score of 0.500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture #2: Four layer model with sigmoid activation functions\n",
    "Now, we add a neuron layer to see how it impacts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 50)                1450      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,053\n",
      "Trainable params: 4,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.6915 - acc: 0.5295\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6915 - acc: 0.5296\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6915 - acc: 0.5296\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6915 - acc: 0.5296\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6914 - acc: 0.5296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.50537715464564459"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 layers. Neurons: 50, 50, 1, 1\n",
    "\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four layer model (50, 50, 1, 1 neurons) with sigmoid activation functions produced an ROC score of 0.505."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture #2: Four layer model with softplus activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_165 (Dense)            (None, 50)                1450      \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,053\n",
      "Trainable params: 4,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 11s 10us/step - loss: 0.6917 - acc: 0.5293\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.6915 - acc: 0.5296\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.6915 - acc: 0.5296\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.6915 - acc: 0.5296\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.6915 - acc: 0.5296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51575275231043649"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change activation to \"softplus\"\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(50, kernel_initializer='uniform'))\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform')) \n",
    "model.add(Activation('softplus'))\n",
    "model.summary()\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four layer model (50, 50, 1, 1 neurons) with softplus activation functions produced an ROC score of 0.516."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture #3: Two layer model with sigmoid activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_152 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.6629 - acc: 0.5933\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 6s 6us/step - loss: 0.6469 - acc: 0.6195\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 7s 6us/step - loss: 0.6379 - acc: 0.6322\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 7s 6us/step - loss: 0.6304 - acc: 0.6422\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 7s 6us/step - loss: 0.6250 - acc: 0.6485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72343445943990137"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 layers. Neurons: 100, 1\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two layer model (100, 1 neurons) with sigmoid activation functions produced an ROC score of 0.723."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture #3: Two layer model with softplus and relu activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_171 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 8.5364 - acc: 0.4704\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 7s 6us/step - loss: 8.5364 - acc: 0.4704\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 8.5364 - acc: 0.4704\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 7s 6us/step - loss: 8.5364 - acc: 0.4704\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 7s 6us/step - loss: 8.5364 - acc: 0.4704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change activation to \"softplus\" and \"relu\"\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two layer model (100, 1 neurons) with softplus and relu activation functions produced an ROC score of 0.500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this initial analysis, the two layer sigmoid model performed the best with an ROC of 0.723.\n",
    "\n",
    "As the number of neuron layers increased, the ROC scores of the models decreases. In the first and third architectures, changing the activation function to something other than sigmoid decreased the ROC score of the model. However, the softplus activation function did slightly improve the ROC score for the four layer model.\n",
    "\n",
    "Not only did the two layer model have the best ROC score, but it performed the quickest as well. The time to run per epoch hovered around 6-7 seconds for the two layer model, whereas the lower scoring three- and four-layer models took between 7-20 seconds per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work # 3: Batch Size Variation\n",
    "In this section, we will vary the batch size of our best model, the two layer model with sigmoid activation functions, and look at the results. For the original model, a batch size of 1,000 was used. Batch sizes of 10 and 100,000 will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_185 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 178s 170us/step - loss: 0.6197 - acc: 0.6573\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 182s 174us/step - loss: 0.6002 - acc: 0.6792\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 182s 173us/step - loss: 0.5926 - acc: 0.6857\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 185s 176us/step - loss: 0.5871 - acc: 0.6906\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 182s 173us/step - loss: 0.5829 - acc: 0.6937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78324926275445317"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arch 3\n",
    "# Batch sizes\n",
    "## 1,000 = 0.7234    (original)\n",
    "## 100,000 = 0.5938\n",
    "## 10 = 0.7832       (takes a long time to run)\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=10)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest batch size, 10, produced the best ROC score for the model of 0.7832. Compared to the original score of 0.7234, the smaller batch size produced a noticeable improvement. However, the smaller batch size drastically increased the time for each epoch of the model, hovering around 180 seconds per epoch to complete. As batch sizes decrease further, the model may improve, but at the cost of time and increasing computing resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work # 4: Different Kernel Initializers\n",
    "Using our best model so far, the two layer model with sigmoid activation functions with a more reasonable batch size of 1,000, we will use three different kernel initializers and examine the impact on ROC scores. Initializations define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "Sixteen basic initializers exist in the Keras API that can be used in the models:\n",
    "* Initializer\n",
    "* Zeros\n",
    "* Ones\n",
    "* Constant\n",
    "* RandomNormal\n",
    "* TruncatedNormal\n",
    "* VarianceScaling\n",
    "* Orthogonal\n",
    "* Identity\n",
    "* lecun_uniform\n",
    "* glorot_normal\n",
    "* glorot_uniform\n",
    "* he_normal\n",
    "* lecun_normal\n",
    "* he_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_206 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 10s 9us/step - loss: 0.6563 - acc: 0.6065\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6363 - acc: 0.6379\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6269 - acc: 0.6493\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6195 - acc: 0.6566\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 7s 7us/step - loss: 0.6143 - acc: 0.6624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74145987549073533"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arch 3\n",
    "# Initializers\n",
    "## uniform = 0.7234 (original)\n",
    "## lecun_uniform = 0.7385\n",
    "## normal = 0.7301\n",
    "## identity = (can only be used for 2D square matrices)\n",
    "## orthogonal = 0.7394\n",
    "## zero = 0.7200\n",
    "## one = 0.5\n",
    "## glorot_normal = 0.7412\n",
    "## glorot_uniform = 0.7342\n",
    "## he_normal = 0.7405\n",
    "## he_uniform = 0.7415 (top score)\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='he_uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='he_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing all of the initializers, the ROC scores for the model are as follows:\n",
    "- uniform = 0.7234 (original)\n",
    "- lecun_uniform = 0.7385\n",
    "- normal = 0.7301\n",
    "- identity = N/A (can only be used for 2D square matrices)\n",
    "- orthogonal = 0.7394\n",
    "- zero = 0.7200\n",
    "- one = 0.500\n",
    "- glorot_normal = 0.7412\n",
    "- glorot_uniform = 0.7342\n",
    "- he_normal = 0.7405\n",
    "- he_uniform = 0.7415 *best score\n",
    "\n",
    "From the scores, several of the initializers produced ROC scores that were similar. However, the he_uniform initializer produced a slightly higher score of 0.7415, edging out glorot_normal and he_normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work # 5: Different Kernel Optimizers\n",
    "Similar to the previous section, using our best model so far, the two layer model with sigmoid activation functions with a more reasonable batch size of 1,000, we will now use three different kernel optimizers and examine the impact on ROC scores. \n",
    "\n",
    "Eight basic initializers exist in the Keras API that can be used in the models, but only seven can be tested:\n",
    "* SGD\n",
    "* RMSprop\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* Adam\n",
    "* Adamax\n",
    "* Nadam\n",
    "* TFOptimizer (wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_256 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1050000/1050000 [==============================] - 11s 10us/step - loss: 7.4927 - acc: 0.5295\n",
      "Epoch 2/5\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 3/5\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 4/5\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 7.4990 - acc: 0.5296\n",
      "Epoch 5/5\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 7.4990 - acc: 0.5296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arch 3\n",
    "# Optimizers\n",
    "## SGD = 0.7234 (original)\n",
    "## SGD = 0.7217 (change decay=0.0)\n",
    "## SGD = 0.6663 (Change momentum=0.2)\n",
    "## SGD = 0.7235 (Change nesterov=False)\n",
    "### RMSprop: recommends leaving params at their default, except for learning rate\n",
    "## RMSprop = 0.7038 (lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "## RMSprop = 0.7050 (lr=0.1, rho=0.9, epsilon=None, decay=0.0)\n",
    "## RMSprop = 0.6280 (lr=0.1, rho=0.2, epsilon=None, decay=0.0)\n",
    "## RMSprop = 0.6721 (lr=0.1, rho=0.9, epsilon=0.5, decay=0.0)\n",
    "## RMSprop = 0.6389 (lr=0.1, rho=0.9, epsilon=None, decay=1e-6)\n",
    "### Adagrad: recommends leaving params at their default\n",
    "## Adagrad = 0.6757 (lr=0.01, epsilon=None, decay=0.0)\n",
    "## Adagrad = 0.7463 (lr=0.1, epsilon=None, decay=0.0) \n",
    "### Adadelta: recommends leaving params at their default\n",
    "## Adadelta = 0.6883 (lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "### Adam\n",
    "## Adam = 0.7085 (lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "## Adam = 0.7614 (lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) \n",
    "## Adam = 0.7497 (lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "### Adamax\n",
    "## Adamax = 0.6928 (lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "## Adamax = 0.7671 (lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0) ******\n",
    "### Nadam: recommends leaving params at their default\n",
    "## Nadam = 0.7410 (lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "## Nadam = 0.7561 (lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#rms = RMSprop(lr=0.1, rho=0.9, epsilon=None, decay=0.0)\n",
    "#adagrad = Adagrad(lr=0.1, epsilon=None, decay=0.0)\n",
    "#adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "#adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#adamax = Adamax(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam = Nadam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=nadam)\n",
    "\n",
    "model.fit(x, y, epochs=5, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing all the optimizers, the ROC score ranges are as follows:\n",
    "\n",
    "* SGD = [0.6663, 0.7235]\n",
    "* RMSprop = [0.6280, 0.7050]\n",
    "* Adagrad = [0.6757, 0.7463]\n",
    "* Adam = [0.7085, 0.7614] \n",
    "* Adamax = [0.6928, 0.7671] *best score\n",
    "* Nadam = [0.7410, 0.7561]\n",
    "\n",
    "From the scores, several of the optimizers produced ROC scores that were similar. However, the Adamax optimizer produced a slightly higher score of 0.7671, edging out Adam and Nadam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work # 6: Produce the best ROC score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the information and insight we have gained from the previous parameter experiments, we will attempt to produce a high ROC score. Batch size and number of epochs will be manipulated to produce the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_266 (Dense)            (None, 100)               2900      \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,001\n",
      "Trainable params: 3,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1050000/1050000 [==============================] - 11s 11us/step - loss: 0.6584 - acc: 0.6110\n",
      "Epoch 2/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.6091 - acc: 0.6662\n",
      "Epoch 3/20\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.5963 - acc: 0.6807\n",
      "Epoch 4/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5886 - acc: 0.6878\n",
      "Epoch 5/20\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.5820 - acc: 0.6933\n",
      "Epoch 6/20\n",
      "1050000/1050000 [==============================] - 9s 9us/step - loss: 0.5786 - acc: 0.6961\n",
      "Epoch 7/20\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.5756 - acc: 0.6986\n",
      "Epoch 8/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5734 - acc: 0.7003\n",
      "Epoch 9/20\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.5715 - acc: 0.7015\n",
      "Epoch 10/20\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.5697 - acc: 0.7030\n",
      "Epoch 11/20\n",
      "1050000/1050000 [==============================] - 9s 9us/step - loss: 0.5681 - acc: 0.7047\n",
      "Epoch 12/20\n",
      "1050000/1050000 [==============================] - 9s 8us/step - loss: 0.5673 - acc: 0.7054\n",
      "Epoch 13/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5667 - acc: 0.7058\n",
      "Epoch 14/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5657 - acc: 0.7065\n",
      "Epoch 15/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5647 - acc: 0.7071\n",
      "Epoch 16/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5636 - acc: 0.7075\n",
      "Epoch 17/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5625 - acc: 0.7086\n",
      "Epoch 18/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5616 - acc: 0.7094\n",
      "Epoch 19/20\n",
      "1050000/1050000 [==============================] - 8s 8us/step - loss: 0.5604 - acc: 0.7102\n",
      "Epoch 20/20\n",
      "1050000/1050000 [==============================] - 8s 7us/step - loss: 0.5600 - acc: 0.7105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79691400584015892"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arch 3\n",
    "## Adamax = 0.7671 (oringal)\n",
    "## Adamax, change epoch 7 = 0.7725 \n",
    "## Adamax, change batch size 10 = 0.7492\n",
    "## Adamax, change epoch 10 = 0.7881 \n",
    "## Adamax, change epoch 10 = 0.7969 ******\n",
    "## Adam, change epoch 7 = 0.7690\n",
    "\n",
    "#Begin \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], kernel_initializer='uniform')) # X_train.shape[1] == 28 here\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(1, kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax = Adamax(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=adamax)\n",
    "\n",
    "model.fit(x, y, epochs=20, batch_size=1000)\n",
    "roc_auc_score(y_test,model.predict(x_test))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with our previous model with the highest ROC score, the two layer model using sigmoid activation functions with a batch size of 1,000, we increased the number of epochs and decreased the batch size to improve the ROC score. Ultimately, we found that increasing the epochs more impactful than decreasing the batch size.\n",
    "\n",
    "With a starting ROC of 0.7671 from the two layer model using the Adamax optimizer, we increased the epochs to 20 and left the batch size at 1,000 to produce our highest ROC score of 0.7969."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Results\"></a>Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "Based on the exercises performed in the Work sections above, we will answer questions about neural networks based on our results.\n",
    "\n",
    "##### Q1: What was the effect of adding more layers/neurons?\n",
    "As the number of neuron layers increased, the ROC scores of the models decreases for our data. As the model adds layers to the neural network, it adds dimensionality. However, the chances of overfitting increase with the additional layers, which diminish performance – in speed and ROC score.\n",
    "\n",
    "##### Q2: Which parameters gave you the best result and why (in your opinion) did they work?\n",
    "A two-layer model (100, 1 neurons) with sigmoid activation functions, Adamax optimizer, 20 epochs and a batch size of 1,000 produced the best results for our model. In terms of the biggest ROC score improvement, selecting two layers for the model was the most impactful parameter selection. However, once the model was fine-tuned in terms of a majority of the parameters, increasing the number of epochs had the biggest impact in the late stage improvement of the ROC score for the model.\n",
    "\n",
    "##### Q3: For work item 6, how did you decide that your model was 'done'?\n",
    "When assessing at what point our final model was 'done' we looked at two main factors: loss/accuracy scores at each epoch and ROC score (an additional factor would also be time taken to run, however, with our two layer architecture and batch sizes this was not an issue for this model). For the ROC score we knew from prior discussion that we were going to attempt to achieve close to 80%. For the loss and accuracy scores at each iteration we wanted to see that loss continued to decrease and that accuracy continued to increase. While each of these scores were continuing in the correct directions when we stopped our model at 20 epochs, the returns were diminishing. Given these narrow gains, and the fact that our ROC score was close to our target (with non-significant increases at further epochs), we concluded that the model had achieved is desired goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Conclusion\"></a>Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we were able to explore the Keras neural network framework and analyze the Higgs data set. By optimizing parameters on our computing resources, we were able to achieve a model with an ROC score of 0.796. While the benchmark for the lab was to produce a model with an ROC score of 0.88, that score was not a realistic threshold for our group. By achieving more than 90% of that ROC score, we consider our model a success.\n",
    "\n",
    "Future work for this analysis would be to further explore the parameters available in the Keras API. There are so many parameters that can be tuned to create unique models, including different Keras layer types. Additionally, with more computing resources, we could reduce the batch size, increase the epochs, and allow the model to churn longer to see if we could squeeze more performance out of the ROC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"References\"></a>References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014).\n",
    "\n",
    "[2] Chollet, Francois, and others. \"Keras.\" 2015. https://keras.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
